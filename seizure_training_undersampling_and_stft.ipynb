{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchvision.transforms as T\n",
    "from transformers import get_scheduler, AdamW\n",
    "from datasets import load_metric\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from scipy.fftpack import dct\n",
    "import IPython.display\n",
    "import librosa\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import pandas as pd;\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import mne\n",
    "import mne.channels\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: Create Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Absence Seizure Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsenceDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None, target_transform=None):\n",
    "        self.x, self.y = self.data_get(file_path)\n",
    "        self.n_samples = len(self.x)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_x, sample_y = self.x[idx], self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample_x = self.transform(sample_x)\n",
    "\n",
    "        if self.target_transform:\n",
    "            sample_y = self.target_transform(sample_y)\n",
    "\n",
    "        return sample_x.float(), sample_y.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def data_get(self, file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        x_loaded, y_loaded = [], []\n",
    "        for index, row in data.iterrows():\n",
    "            a = np.load(row[\"labels\"])\n",
    "            for i, v in enumerate(a):\n",
    "                a[i] = int(v)\n",
    "            y_loaded.append(a)\n",
    "\n",
    "            x_loaded.append(np.load(row[\"data\"]))\n",
    "\n",
    "        continuous_x = np.concatenate(x_loaded, axis=0)\n",
    "        continuous_y = np.concatenate(y_loaded, axis=0)\n",
    "\n",
    "        return continuous_x, continuous_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load the dataset\n",
    "dataset = AbsenceDataset(\n",
    "    \"processed_data/absence/five_channels_il_075/epochs_files.csv\",\n",
    "    transform=torch.tensor,\n",
    "    target_transform=torch.tensor,\n",
    ")\n",
    "dataset.y = dataset.y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4141, 11, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n",
    "\n",
    "70%, 20%, 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_len = len(dataset)\n",
    "dataset_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split: 2899\n",
      "Eval Split: 828\n",
      "Test Split: 414\n",
      "Total: 4141\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    int(dataset_len * 0.7) + int(dataset_len * 0.2) + int(dataset_len * 0.1)\n",
    ") != dataset_len:\n",
    "    train_set, eval_set, test_set = random_split(\n",
    "        dataset,\n",
    "        [\n",
    "            math.ceil(dataset_len * 0.7),\n",
    "            int(dataset_len * 0.2),\n",
    "            int(dataset_len * 0.1),\n",
    "        ],\n",
    "    )\n",
    "else:\n",
    "    train_set, eval_set, test_set = random_split(\n",
    "        dataset,\n",
    "        [\n",
    "            int(dataset_len * 0.1),\n",
    "            int(dataset_len * 0.7),\n",
    "            int(dataset_len * 0.2),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(f\"Train Split: {len(train_set)}\")\n",
    "print(f\"Eval Split: {len(eval_set)}\")\n",
    "print(f\"Test Split: {len(test_set)}\")\n",
    "print(f\"Total: {len(train_set) + len(eval_set) + len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2899, 11, 1000), (2899,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.dataset.x[train_set.indices].shape, train_set.dataset.y[\n",
    "    train_set.indices\n",
    "].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background: 2784 // Absence Seizure: 115\n"
     ]
    }
   ],
   "source": [
    "bk, sz = 0, 0\n",
    "for i, v in enumerate(train_set.dataset.y[train_set.indices]):\n",
    "    if v == 1:\n",
    "        sz += 1\n",
    "    else:\n",
    "        bk += 1\n",
    "\n",
    "print(f\"Background: {bk} // Absence Seizure: {sz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2899, 11000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = train_set.dataset.x[train_set.indices].shape\n",
    "data_reshaped = train_set.dataset.x[train_set.indices]\n",
    "data_reshaped = np.reshape(data_reshaped, (shape[0], shape[1] * shape[2]))\n",
    "data_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_us = RandomUnderSampler(random_state=42)\n",
    "train_set_x, train_set_y = rand_us.fit_resample(\n",
    "    data_reshaped, train_set.dataset.y[train_set.indices]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 11, 1000), (828, 11, 1000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x = np.reshape(train_set_x, (train_set_x.shape[0], shape[1], shape[2]))\n",
    "val_set_x = eval_set.dataset.x[eval_set.indices]\n",
    "train_set_x.shape, val_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background: 115 // Absence Seizure: 115\n"
     ]
    }
   ],
   "source": [
    "bk, sz = 0, 0\n",
    "for i, v in enumerate(train_set_y):\n",
    "    if v == 1:\n",
    "        sz += 1\n",
    "    else:\n",
    "        bk += 1\n",
    "\n",
    "print(f\"Background: {bk} // Absence Seizure: {sz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STFT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create STFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 11, 4, 6), (828, 11, 4, 6))"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fft = int(1024 / 6)\n",
    "window = \"taylor\"\n",
    "hop_length = int(n_fft)\n",
    "\n",
    "# Train\n",
    "stft_channel_data = []\n",
    "for i, channels in enumerate(train_set_x):\n",
    "    stft_channel_data.append(\n",
    "        (\n",
    "            librosa.feature.melspectrogram(\n",
    "                y=channels,\n",
    "                sr=250,\n",
    "                n_mels=4,\n",
    "                n_fft=n_fft,\n",
    "                fmin=0,\n",
    "                fmax=4,\n",
    "                hop_length=hop_length,\n",
    "                window=window,\n",
    "                center=True,\n",
    "                pad_mode=\"reflect\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "stft_channel_data = np.array(stft_channel_data)\n",
    "\n",
    "# Val\n",
    "val_stft_channel_data = []\n",
    "for i, channels in enumerate(val_set_x):\n",
    "    val_stft_channel_data.append(\n",
    "        (\n",
    "            librosa.feature.melspectrogram(\n",
    "                y=channels,\n",
    "                sr=250,\n",
    "                n_mels=4,\n",
    "                n_fft=n_fft,\n",
    "                fmin=0,\n",
    "                fmax=5,\n",
    "                hop_length=hop_length,\n",
    "                window=window,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "val_stft_channel_data = np.array(val_stft_channel_data)\n",
    "\n",
    "stft_channel_data.shape, val_stft_channel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 11, 4, 6), (828, 11, 4, 6))"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "train_db = []\n",
    "for i, channels in enumerate(stft_channel_data):\n",
    "    train_db.append(librosa.amplitude_to_db((channels), ref=np.max))\n",
    "train_db = np.array(train_db)\n",
    "\n",
    "# Val\n",
    "val_db = []\n",
    "for i, channels in enumerate(val_stft_channel_data):\n",
    "    val_db.append(librosa.amplitude_to_db((channels), ref=np.max))\n",
    "val_db = np.array(val_db)\n",
    "\n",
    "train_db.shape, val_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 4, 6), (828, 4, 6))"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "train_db_mean = []\n",
    "for i, channels in enumerate(stft_channel_data):\n",
    "    train_db_mean.append(np.mean(channels, axis=0))\n",
    "train_db_mean = np.array(train_db_mean)\n",
    "\n",
    "# Val\n",
    "val_db_mean = []\n",
    "for i, channels in enumerate(val_stft_channel_data):\n",
    "    val_db_mean.append(np.mean(channels, axis=0))\n",
    "val_db_mean = np.array(val_db_mean)\n",
    "\n",
    "train_db_mean.shape, val_db_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "QuadMesh.set() got an unexpected keyword argument 'center'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[616], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_axis_off()\n\u001b[0;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39madd_axes(ax)\n\u001b[1;32m----> 8\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_process.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_process.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\librosa\\display.py:1208\u001b[0m, in \u001b[0;36mspecshow\u001b[1;34m(data, x_coords, y_coords, x_axis, y_axis, sr, hop_length, n_fft, win_length, fmin, fmax, tuning, bins_per_octave, key, Sa, mela, thaat, auto_aspect, htk, unicode, intervals, unison, ax, **kwargs)\u001b[0m\n\u001b[0;32m   1204\u001b[0m x_coords \u001b[38;5;241m=\u001b[39m __mesh_coords(x_axis, x_coords, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_params)\n\u001b[0;32m   1206\u001b[0m axes \u001b[38;5;241m=\u001b[39m __check_axes(ax)\n\u001b[1;32m-> 1208\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43maxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpcolormesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m __set_current_image(ax, out)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# Set up axis scaling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:6295\u001b[0m, in \u001b[0;36mAxes.pcolormesh\u001b[1;34m(self, alpha, norm, cmap, vmin, vmax, shading, antialiased, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6291\u001b[0m coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([X, Y], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6293\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnap\u001b[39m\u001b[38;5;124m'\u001b[39m, mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpcolormesh.snap\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 6295\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mmcoll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuadMesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialiased\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialiased\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6297\u001b[0m \u001b[43m    \u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6298\u001b[0m collection\u001b[38;5;241m.\u001b[39m_scale_norm(norm, vmin, vmax)\n\u001b[0;32m   6300\u001b[0m coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# flatten the grid structure; keep x, y\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\matplotlib\\collections.py:2164\u001b[0m, in \u001b[0;36mQuadMesh.__init__\u001b[1;34m(self, coordinates, antialiased, shading, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickradius\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(coordinates\u001b[38;5;241m=\u001b[39mcoordinates, shading\u001b[38;5;241m=\u001b[39mshading)\n\u001b[1;32m-> 2164\u001b[0m \u001b[43mCollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_antialiased \u001b[38;5;241m=\u001b[39m antialiased\n\u001b[0;32m   2167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bbox \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mBbox\u001b[38;5;241m.\u001b[39munit()\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\matplotlib\\collections.py:203\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, offset_transform, norm, cmap, pickradius, hatch, urls, zorder, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset_transform \u001b[38;5;241m=\u001b[39m offset_transform\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path_effects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\matplotlib\\artist.py:1219\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;124;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[38;5;124;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\matplotlib\\artist.py:1193\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[1;34m(self, props, errfmt)\u001b[0m\n\u001b[0;32m   1191\u001b[0m             func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1192\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[1;32m-> 1193\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1194\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk))\n\u001b[0;32m   1195\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(func(v))\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
      "\u001b[1;31mAttributeError\u001b[0m: QuadMesh.set() got an unexpected keyword argument 'center'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% capture\n",
    "image_train = []\n",
    "for i, data in enumerate(train_db_mean):\n",
    "    f = plt.figure(frameon=False)\n",
    "    ax = plt.Axes(f, [0.0, 0.0, 1.0, 1.0])\n",
    "    ax.set_axis_off()\n",
    "    f.add_axes(ax)\n",
    "    img = librosa.display.specshow(data, sr=250, x_axis=\"time\", y_axis=\"mel\")\n",
    "    plt.savefig(\"image_process.png\")\n",
    "\n",
    "    img = Image.open(\"image_process.png\")\n",
    "    img = img.resize((120, 160))\n",
    "    image_train.append(img)\n",
    "    plt.close(\"all\")\n",
    "image_train = np.array(image_train)\n",
    "\n",
    "image_val = []\n",
    "for i, data in enumerate(val_db_mean):\n",
    "    f = plt.figure(frameon=False)\n",
    "    ax = plt.Axes(f, [0.0, 0.0, 1.0, 1.0])\n",
    "    ax.set_axis_off()\n",
    "    f.add_axes(ax)\n",
    "    img = librosa.display.specshow(data, sr=250, x_axis=\"time\", y_axis=\"mel\")\n",
    "    plt.savefig(\"image_process.png\")\n",
    "\n",
    "    img = Image.open(\"image_process.png\")\n",
    "    img = img.resize((120, 160))\n",
    "    image_val.append(img)\n",
    "    plt.close(\"all\")\n",
    "image_val = np.array(image_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 160, 120, 4), (828, 160, 120, 4))"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape, image_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 4, 160, 120), (828, 4, 160, 120))"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_shape = []\n",
    "for i, image in enumerate(image_train):\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    fix_shape.append(image)\n",
    "fix_shape = np.array(fix_shape)\n",
    "\n",
    "fix_shape_val = []\n",
    "for i, image in enumerate(image_val):\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    fix_shape_val.append(image)\n",
    "fix_shape_val = np.array(fix_shape_val)\n",
    "\n",
    "image_train = fix_shape\n",
    "image_val = fix_shape_val\n",
    "\n",
    "fix_shape.shape, fix_shape_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[617], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m166\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimage_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m image_train[x]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_set_y[x])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "x = 166\n",
    "print(image_train[0].shape)\n",
    "\n",
    "a = image_train[x].transpose(1, 2, 0)\n",
    "print(train_set_y[x])\n",
    "\n",
    "Image.fromarray(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display STFT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remake Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_transform=None):\n",
    "        self.x, self.y = data\n",
    "        self.n_samples = len(self.x)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_x, sample_y = self.x[idx], self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample_x = self.transform(sample_x)\n",
    "\n",
    "        if self.target_transform:\n",
    "            sample_y = self.target_transform(sample_y)\n",
    "\n",
    "        return sample_x.float(), sample_y.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_balanced_train_dataset = BalancedDataset(\n",
    "    (image_train, train_set_y), transform=torch.tensor, target_transform=torch.tensor\n",
    ")\n",
    "mfcc_eval_dataset = BalancedDataset(\n",
    "    (image_val, eval_set.dataset.y[eval_set.indices]),\n",
    "    transform=torch.tensor,\n",
    "    target_transform=torch.tensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 of each class\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 5))\n",
    "fig.suptitle(\n",
    "    \"3 samples of each class: Absence Seizure and Background - Plotted as images with values scaled to 0-255\"\n",
    ")\n",
    "\n",
    "c1 = 0\n",
    "c0 = 0\n",
    "\n",
    "for i, v in enumerate(train_set.dataset.y[train_set.indices]):\n",
    "    if v == 0 and c0 < 3:\n",
    "        clone = train_set.dataset.x[train_set.indices][i]\n",
    "        max_pixel = 0\n",
    "        min_pixel = 0\n",
    "        for m, b in enumerate(clone):\n",
    "            for j, n in enumerate(clone[m]):\n",
    "                if n > max_pixel:\n",
    "                    max_pixel = n\n",
    "                if n < min_pixel:\n",
    "                    min_pixel = n\n",
    "\n",
    "        for m, b in enumerate(clone):\n",
    "            for j, n in enumerate(clone[m]):\n",
    "                a = clone[m][j] - min_pixel\n",
    "                b = 255 / (max_pixel - min_pixel)\n",
    "                norm = a * b\n",
    "                if norm > 255:\n",
    "                    norm = 255\n",
    "                if norm < 0:\n",
    "                    norm = 0\n",
    "                clone[m][j] = norm\n",
    "\n",
    "        # Increase image size for better visibility if needed\n",
    "        clone = np.repeat(clone, 50, axis=0)\n",
    "\n",
    "        axs[0, c0].imshow(clone, cmap=\"gray\")\n",
    "        axs[0, c0].set_title(f\"Label 0: Background // Data Index: {i}\")\n",
    "        axs[0, c0].set_yticks([])\n",
    "        axs[0, c0].set_xticks([0, 1000])\n",
    "        axs[0, c0].set_xlabel(\"Data Points Per Window\")\n",
    "\n",
    "        c0 += 1\n",
    "\n",
    "    if v == 1 and c1 < 3:\n",
    "        clone = train_set.dataset.x[train_set.indices][i]\n",
    "        max_pixel = 0\n",
    "        min_pixel = 0\n",
    "\n",
    "        for m, b in enumerate(clone):\n",
    "            for j, n in enumerate(clone[m]):\n",
    "                if n > max_pixel:\n",
    "                    max_pixel = n\n",
    "                if n < min_pixel:\n",
    "                    min_pixel = n\n",
    "\n",
    "        for m, b in enumerate(clone):\n",
    "            for j, n in enumerate(clone[m]):\n",
    "                a = clone[m][j] - min_pixel\n",
    "                b = 255 / (max_pixel - min_pixel)\n",
    "                norm = a * b\n",
    "                if norm > 255:\n",
    "                    norm = 255\n",
    "                if norm < 0:\n",
    "                    norm = 0\n",
    "                clone[m][j] = norm\n",
    "\n",
    "        # Increase image size for better visibility if needed\n",
    "        clone = np.repeat(clone, 50, axis=0)\n",
    "\n",
    "        axs[1, c1].imshow(clone, cmap=\"gray\")\n",
    "        axs[1, c1].set_title(f\"Label 1: Absence Seizure // Data Index: {i}\")\n",
    "        axs[1, c1].set_yticks([])\n",
    "        axs[1, c1].set_xticks([0, 1000])\n",
    "        axs[1, c1].set_xlabel(\"Data Points Per Window\")\n",
    "\n",
    "        c1 += 1\n",
    "\n",
    "    if c1 == 3 and c0 == 3:\n",
    "        break\n",
    "\n",
    "    # new_img = Image.fromarray(clone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 of each class\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "fig.suptitle(\"3 samples of each class\")\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i, v in enumerate(train_set.dataset.x[train_set.indices]):\n",
    "    if c0 < 3 and train_set.dataset.y[train_set.indices][i] == 0:\n",
    "        axs[0, c0].plot(train_set.dataset.x[train_set.indices][i][0], label=\"Channel 1\")\n",
    "        axs[0, c0].plot(train_set.dataset.x[train_set.indices][i][1], label=\"Channel 2\")\n",
    "        axs[0, c0].plot(train_set.dataset.x[train_set.indices][i][2], label=\"Channel 3\")\n",
    "        axs[0, c0].plot(train_set.dataset.x[train_set.indices][i][3], label=\"Channel 4\")\n",
    "        axs[0, c0].plot(train_set.dataset.x[train_set.indices][i][4], label=\"Channel 5\")\n",
    "        axs[0, c0].legend(loc=\"upper right\")\n",
    "        axs[0, c0].set_title(f\"Label 0: Background // Data Index: {i}\")\n",
    "        c0 += 1\n",
    "    elif c1 < 3 and train_set.dataset.y[train_set.indices][i] == 1:\n",
    "        axs[1, c1].plot(train_set.dataset.x[train_set.indices][i][0], label=\"Channel 1\")\n",
    "        axs[1, c1].plot(train_set.dataset.x[train_set.indices][i][1], label=\"Channel 2\")\n",
    "        axs[1, c1].plot(train_set.dataset.x[train_set.indices][i][2], label=\"Channel 3\")\n",
    "        axs[1, c1].plot(train_set.dataset.x[train_set.indices][i][3], label=\"Channel 4\")\n",
    "        axs[1, c1].plot(train_set.dataset.x[train_set.indices][i][4], label=\"Channel 5\")\n",
    "        axs[1, c1].legend(loc=\"upper right\")\n",
    "        axs[1, c1].set_title(f\"Label 1: Absence Seizure // Data Index: {i}\")\n",
    "        c1 += 1\n",
    "\n",
    "    if c0 == 3 and c1 == 3:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 of each class\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "fig.suptitle(\"3 samples of each class\")\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i, v in enumerate(x_train_reshaped):\n",
    "    if c0 < 3 and train_set.dataset.y[train_set.indices][i] == 0:\n",
    "        axs[0, c0].plot(x_train_reshaped[i])\n",
    "        axs[0, c0].set_title(f\"Label 0: Background // Data Index: {i}\")\n",
    "        c0 += 1\n",
    "    elif c1 < 3 and train_set.dataset.y[train_set.indices][i] == 1:\n",
    "        axs[1, c1].plot(x_train_reshaped[i])\n",
    "        axs[1, c1].set_title(f\"Label 1: Absence Seizure // Data Index: {i}\")\n",
    "        c1 += 1\n",
    "\n",
    "    if c0 == 3 and c1 == 3:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=new_balanced_train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "eval_dataloader = DataLoader(dataset=mfcc_eval_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 13,582\n"
     ]
    }
   ],
   "source": [
    "class AbnormalNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input 5 Channels\n",
    "        self.CNN1 = nn.Conv1d(in_channels=13, out_channels=20, kernel_size=3)\n",
    "        self.MaxPool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.CNN2 = nn.Conv1d(in_channels=20, out_channels=60, kernel_size=3)\n",
    "        self.MaxPool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.CNN3 = nn.Conv1d(in_channels=60, out_channels=1, kernel_size=2)\n",
    "        self.MaxPool3 = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "\n",
    "        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x38 and 6x100)\n",
    "        self.fc1 = nn.Linear(in_features=38, out_features=100)\n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=50)\n",
    "        self.output = nn.Linear(in_features=50, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.CNN1(x))\n",
    "        x = self.MaxPool1(x)\n",
    "\n",
    "        x = F.relu(self.CNN2(x))\n",
    "        x = self.MaxPool2(x)\n",
    "\n",
    "        x = F.relu(self.CNN3(x))\n",
    "        x = self.MaxPool3(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set device to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = AbnormalNeuralNetwork().to(device)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Calculate number of parameters in model\n",
    "n_params = 0\n",
    "for x in model.parameters():\n",
    "    n_params += len(torch.flatten(x))\n",
    "print(f\"Number of parameters in model: {n_params:,}\")\n",
    "\n",
    "# Print Model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 1,395,266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AbnormalNeuralNetwork(\n",
       "  (CNN1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
       "  (MaxPool1): MaxPool2d(kernel_size=(6, 6), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (CNN2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
       "  (MaxPool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (CNN3): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
       "  (MaxPool3): MaxPool2d(kernel_size=(1, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=260, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (output): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AbnormalNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input 5 Channels\n",
    "        self.CNN1 = nn.Conv2d(\n",
    "            in_channels=4, out_channels=64, kernel_size=3, padding=(1, 0)\n",
    "        )\n",
    "        self.MaxPool1 = nn.MaxPool2d(kernel_size=(6, 6), stride=2)\n",
    "\n",
    "        self.CNN2 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, padding=(1, 0)\n",
    "        )\n",
    "        self.MaxPool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.CNN3 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=1, kernel_size=3, padding=(1, 0)\n",
    "        )\n",
    "        self.MaxPool3 = nn.MaxPool2d(kernel_size=(1, 1), stride=2)\n",
    "\n",
    "        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x260 and 5043x1024)\n",
    "        self.fc1 = nn.Linear(260, 1024)\n",
    "        # self.dropout1 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        # self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output = nn.Linear(in_features=1024, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.CNN1(x))\n",
    "        x = self.MaxPool1(x)\n",
    "\n",
    "        x = F.relu(self.CNN2(x))\n",
    "        x = self.MaxPool2(x)\n",
    "\n",
    "        x = F.relu(self.CNN3(x))\n",
    "        x = self.MaxPool3(x)\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.output(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = AbnormalNeuralNetwork().to(device)\n",
    "\n",
    "# Calculate number of parameters in model\n",
    "n_params = 0\n",
    "for x in model.parameters():\n",
    "    n_params += len(torch.flatten(x))\n",
    "print(f\"Number of parameters in model: {n_params:,}\")\n",
    "\n",
    "# Print Model\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Model 2: CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 1,085,405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (global_avg): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc1): Linear(in_features=128, out_features=164, bias=True)\n",
       "  (fc2): Linear(in_features=164, out_features=164, bias=True)\n",
       "  (fc3): Linear(in_features=164, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=10)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=10)\n",
    "        self.global_avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(128, 164)\n",
    "        self.fc2 = nn.Linear(164, 164)\n",
    "        self.fc3 = nn.Linear(164, 1)\n",
    "\n",
    "    def forward(self, out):\n",
    "        out = self.pool(F.relu(self.conv1(out)))\n",
    "        out = self.pool(F.relu(self.conv2(out)))\n",
    "        out = self.pool(F.relu(self.conv3(out)))\n",
    "        out = self.global_avg(out)\n",
    "        out = torch.flatten(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = Net().to(device)\n",
    "\n",
    "# Calculate number of parameters in model\n",
    "n_params = 0\n",
    "for x in model.parameters():\n",
    "    n_params += len(torch.flatten(x))\n",
    "print(f\"Number of parameters in model: {n_params:,}\")\n",
    "\n",
    "# Print Model\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(39, 78)\n",
    "        self.linear2 = nn.Linear(78, 78)\n",
    "        self.output = nn.Linear(78, 1)\n",
    "\n",
    "    def forward(self, out):\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = F.relu(self.linear2(out))\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LinearRegression().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_params = 0\n",
    "for x in model.parameters():\n",
    "    n_params += len(torch.flatten(x))\n",
    "print(f\"Number of parameters in model: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train // Background: 115 // Absence Seizure: 115\n",
      "Validation // Background: 787 // Absence Seizure: 41\n"
     ]
    }
   ],
   "source": [
    "# Plot distribution of the dataset before balancing\n",
    "# 0 - Background epoch\n",
    "# 1 - Absence seizure epoch\n",
    "\n",
    "bk = 0\n",
    "sz = 0\n",
    "for i, v in enumerate(train_dataloader):\n",
    "    if v[1].squeeze() == torch.tensor([1.0]):\n",
    "        sz += 1\n",
    "    else:\n",
    "        bk += 1\n",
    "\n",
    "print(f\"Train // Background: {bk} // Absence Seizure: {sz}\")\n",
    "\n",
    "bk = 0\n",
    "sz = 0\n",
    "for i, v in enumerate(eval_dataloader):\n",
    "    if v[1].squeeze() == torch.tensor([1.0]):\n",
    "        sz += 1\n",
    "    else:\n",
    "        bk += 1\n",
    "\n",
    "print(f\"Validation // Background: {bk} // Absence Seizure: {sz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 160, 120]) tensor([0.])\n",
      "torch.Size([1, 4, 160, 120]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(eval_dataloader):\n",
    "    first_input = v[0]\n",
    "    first_label = v[1]\n",
    "    break\n",
    "\n",
    "print(first_input.shape, first_label)\n",
    "\n",
    "for i, v in enumerate(train_dataloader):\n",
    "    first_input = v[0]\n",
    "    first_label = v[1]\n",
    "    break\n",
    "\n",
    "print(first_input.shape, first_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_epochs * len(train_dataloader),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69000 [01:09<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n",
    "\n",
    "loss_track = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.squeeze(0)\n",
    "\n",
    "        # print(type(inputs))\n",
    "        # numpy_inputs = inputs.cpu().detach().numpy()\n",
    "        # image = numpy_inputs.transpose(1, 2, 0)\n",
    "        # image = image.astype(\"uint8\")\n",
    "        # image = Image.fromarray(image)\n",
    "        # image.show()\n",
    "        # print(labels)\n",
    "        # break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # outputs = outputs.squeeze(-1)  # Not needed for LR\n",
    "        # outputs = outputs.squeeze(-1)  # Not needed for LR\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    loss_track.append(loss)\n",
    "    # break\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9973], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs_log[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loicl\\Documents\\UniWork\\Year 3\\Dissertation\\TinyML-for-Epileptic-Seizures\\.venv\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3796, device='cuda:0')\n",
      "tensor(0.3229, device='cuda:0')\n",
      "tensor(0.3561, device='cuda:0')\n",
      "tensor(0.3079, device='cuda:0')\n",
      "tensor(0.3478, device='cuda:0')\n",
      "tensor(0.3513, device='cuda:0')\n",
      "tensor(0.3317, device='cuda:0')\n",
      "tensor(0.3275, device='cuda:0')\n",
      "tensor(0.3642, device='cuda:0')\n",
      "tensor(0.3198, device='cuda:0')\n",
      "tensor(0.3752, device='cuda:0')\n",
      "tensor(0.3490, device='cuda:0')\n",
      "tensor(0.3573, device='cuda:0')\n",
      "tensor(0.3578, device='cuda:0')\n",
      "tensor(0.4532, device='cuda:0')\n",
      "tensor(0.3577, device='cuda:0')\n",
      "tensor(0.3466, device='cuda:0')\n",
      "tensor(0.3024, device='cuda:0')\n",
      "tensor(0.2859, device='cuda:0')\n",
      "tensor(0.3214, device='cuda:0')\n",
      "tensor(0.3561, device='cuda:0')\n",
      "tensor(0.3557, device='cuda:0')\n",
      "tensor(0.3567, device='cuda:0')\n",
      "tensor(0.3277, device='cuda:0')\n",
      "tensor(0.3671, device='cuda:0')\n",
      "tensor(0.3228, device='cuda:0')\n",
      "tensor(0.3507, device='cuda:0')\n",
      "tensor(0.3332, device='cuda:0')\n",
      "tensor(0.3740, device='cuda:0')\n",
      "tensor(0.3930, device='cuda:0')\n",
      "tensor(0.3372, device='cuda:0')\n",
      "tensor(0.3474, device='cuda:0')\n",
      "tensor(0.3501, device='cuda:0')\n",
      "tensor(0.3834, device='cuda:0')\n",
      "tensor(0.3512, device='cuda:0')\n",
      "tensor(0.3485, device='cuda:0')\n",
      "tensor(0.3279, device='cuda:0')\n",
      "tensor(0.3899, device='cuda:0')\n",
      "tensor(0.3584, device='cuda:0')\n",
      "tensor(0.3512, device='cuda:0')\n",
      "tensor(0.3995, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "results_list = []\n",
    "for batch in eval_dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    inputs = inputs.squeeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)  # Not needed for LR\n",
    "\n",
    "    results_list.append(outputs)\n",
    "    # print(outputs)\n",
    "    if outputs > 0.5:\n",
    "        if labels == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if labels == 0:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    # logits = outputs.logits\n",
    "    # predictions = torch.argmax(logits, dim=-1)\n",
    "    # metric.add_batch(predictions=outputs, references=labels)\n",
    "\n",
    "# results = metric.compute()\n",
    "for i, v in enumerate(eval_dataloader):\n",
    "    if v[1].squeeze() == torch.tensor([1.0]):\n",
    "        print(results_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 0\n",
      "False Positives: 0\n",
      "True Negatives: 787\n",
      "False Negatives: 41\n",
      "[tensor(0.3467, device='cuda:0'), tensor(0.3404, device='cuda:0'), tensor(0.3549, device='cuda:0'), tensor(0.3458, device='cuda:0'), tensor(0.3343, device='cuda:0'), tensor(0.3309, device='cuda:0'), tensor(0.3506, device='cuda:0'), tensor(0.3133, device='cuda:0'), tensor(0.3479, device='cuda:0'), tensor(0.3609, device='cuda:0'), tensor(0.3611, device='cuda:0'), tensor(0.3495, device='cuda:0'), tensor(0.4138, device='cuda:0'), tensor(0.3857, device='cuda:0'), tensor(0.3482, device='cuda:0'), tensor(0.3357, device='cuda:0'), tensor(0.3575, device='cuda:0'), tensor(0.4087, device='cuda:0'), tensor(0.3451, device='cuda:0'), tensor(0.3289, device='cuda:0'), tensor(0.3691, device='cuda:0'), tensor(0.3707, device='cuda:0'), tensor(0.3654, device='cuda:0'), tensor(0.3586, device='cuda:0'), tensor(0.3551, device='cuda:0'), tensor(0.3562, device='cuda:0'), tensor(0.3591, device='cuda:0'), tensor(0.3327, device='cuda:0'), tensor(0.3006, device='cuda:0'), tensor(0.3497, device='cuda:0'), tensor(0.4261, device='cuda:0'), tensor(0.3489, device='cuda:0'), tensor(0.3285, device='cuda:0'), tensor(0.3609, device='cuda:0'), tensor(0.3537, device='cuda:0'), tensor(0.3123, device='cuda:0'), tensor(0.3415, device='cuda:0'), tensor(0.3590, device='cuda:0'), tensor(0.3570, device='cuda:0'), tensor(0.3448, device='cuda:0'), tensor(0.3022, device='cuda:0'), tensor(0.3366, device='cuda:0'), tensor(0.2353, device='cuda:0'), tensor(0.3461, device='cuda:0'), tensor(0.3651, device='cuda:0'), tensor(0.3202, device='cuda:0'), tensor(0.3486, device='cuda:0'), tensor(0.3591, device='cuda:0'), tensor(0.3333, device='cuda:0'), tensor(0.3189, device='cuda:0'), tensor(0.3340, device='cuda:0'), tensor(0.2493, device='cuda:0'), tensor(0.3512, device='cuda:0'), tensor(0.3354, device='cuda:0'), tensor(0.2452, device='cuda:0'), tensor(0.3535, device='cuda:0'), tensor(0.3597, device='cuda:0'), tensor(0.3247, device='cuda:0'), tensor(0.3570, device='cuda:0'), tensor(0.3513, device='cuda:0'), tensor(0.3608, device='cuda:0'), tensor(0.3303, device='cuda:0'), tensor(0.3426, device='cuda:0'), tensor(0.3765, device='cuda:0'), tensor(0.3299, device='cuda:0'), tensor(0.3776, device='cuda:0'), tensor(0.3529, device='cuda:0'), tensor(0.3371, device='cuda:0'), tensor(0.2972, device='cuda:0'), tensor(0.3605, device='cuda:0'), tensor(0.3665, device='cuda:0'), tensor(0.3213, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.3304, device='cuda:0'), tensor(0.3454, device='cuda:0'), tensor(0.3796, device='cuda:0'), tensor(0.3300, device='cuda:0'), tensor(0.3604, device='cuda:0'), tensor(0.3475, device='cuda:0'), tensor(0.3400, device='cuda:0'), tensor(0.3529, device='cuda:0'), tensor(0.3500, device='cuda:0'), tensor(0.3509, device='cuda:0'), tensor(0.3547, device='cuda:0'), tensor(0.3510, device='cuda:0'), tensor(0.3526, device='cuda:0'), tensor(0.3304, device='cuda:0'), tensor(0.3593, device='cuda:0'), tensor(0.3387, device='cuda:0'), tensor(0.3543, device='cuda:0'), tensor(0.3434, device='cuda:0'), tensor(0.2926, device='cuda:0'), tensor(0.3229, device='cuda:0'), tensor(0.3515, device='cuda:0'), tensor(0.3442, device='cuda:0'), tensor(0.3560, device='cuda:0'), tensor(0.3599, device='cuda:0'), tensor(0.3561, device='cuda:0'), tensor(0.3531, device='cuda:0'), tensor(0.3623, device='cuda:0'), tensor(0.3079, device='cuda:0'), tensor(0.3415, device='cuda:0'), tensor(0.3467, device='cuda:0'), tensor(0.3515, device='cuda:0'), tensor(0.3590, device='cuda:0'), tensor(0.3326, device='cuda:0'), tensor(0.3570, device='cuda:0'), tensor(0.3427, device='cuda:0'), tensor(0.3605, device='cuda:0'), tensor(0.3594, device='cuda:0'), tensor(0.3577, device='cuda:0'), tensor(0.3519, device='cuda:0'), tensor(0.3684, device='cuda:0'), tensor(0.3557, device='cuda:0'), tensor(0.3586, device='cuda:0'), tensor(0.3759, device='cuda:0'), tensor(0.3686, device='cuda:0'), tensor(0.2983, device='cuda:0'), tensor(0.3517, device='cuda:0'), tensor(0.3485, device='cuda:0'), tensor(0.3160, device='cuda:0'), tensor(0.3462, device='cuda:0'), tensor(0.3602, device='cuda:0'), tensor(0.3215, device='cuda:0'), tensor(0.3532, device='cuda:0'), tensor(0.3060, device='cuda:0'), tensor(0.3615, device='cuda:0'), tensor(0.3481, device='cuda:0'), tensor(0.3501, device='cuda:0'), tensor(0.3332, device='cuda:0'), tensor(0.3458, device='cuda:0'), tensor(0.3416, device='cuda:0'), tensor(0.3699, device='cuda:0'), tensor(0.3452, device='cuda:0'), tensor(0.3684, device='cuda:0'), tensor(0.3942, device='cuda:0'), tensor(0.3017, device='cuda:0'), tensor(0.3539, device='cuda:0'), tensor(0.4256, device='cuda:0'), tensor(0.3787, device='cuda:0'), tensor(0.3314, device='cuda:0'), tensor(0.2879, device='cuda:0'), tensor(0.3359, device='cuda:0'), tensor(0.3364, device='cuda:0'), tensor(0.3504, device='cuda:0'), tensor(0.3303, device='cuda:0'), tensor(0.3416, device='cuda:0'), tensor(0.3525, device='cuda:0'), tensor(0.3350, device='cuda:0'), tensor(0.3402, device='cuda:0'), tensor(0.3536, device='cuda:0'), tensor(0.3511, device='cuda:0'), tensor(0.3459, device='cuda:0'), tensor(0.3290, device='cuda:0'), tensor(0.4029, device='cuda:0'), tensor(0.3549, device='cuda:0'), tensor(0.3613, device='cuda:0'), tensor(0.3478, device='cuda:0'), tensor(0.3142, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.3513, device='cuda:0'), tensor(0.3371, device='cuda:0'), tensor(0.3317, device='cuda:0'), tensor(0.3496, device='cuda:0'), tensor(0.3369, device='cuda:0'), tensor(0.3533, device='cuda:0'), tensor(0.3342, device='cuda:0'), tensor(0.3545, device='cuda:0'), tensor(0.3555, device='cuda:0'), tensor(0.3334, device='cuda:0'), tensor(0.3478, device='cuda:0'), tensor(0.3824, device='cuda:0'), tensor(0.3799, device='cuda:0'), tensor(0.3590, device='cuda:0'), tensor(0.3576, device='cuda:0'), tensor(0.3594, device='cuda:0'), tensor(0.3490, device='cuda:0'), tensor(0.3491, device='cuda:0'), tensor(0.3417, device='cuda:0'), tensor(0.3576, device='cuda:0'), tensor(0.3579, device='cuda:0'), tensor(0.3542, device='cuda:0'), tensor(0.3479, device='cuda:0'), tensor(0.3770, device='cuda:0'), tensor(0.3477, device='cuda:0'), tensor(0.3275, device='cuda:0'), tensor(0.3469, device='cuda:0'), tensor(0.3542, device='cuda:0'), tensor(0.3319, device='cuda:0'), tensor(0.3481, device='cuda:0'), tensor(0.3641, device='cuda:0'), tensor(0.3281, device='cuda:0'), tensor(0.3074, device='cuda:0'), tensor(0.2926, device='cuda:0'), tensor(0.3413, device='cuda:0'), tensor(0.3642, device='cuda:0'), tensor(0.3185, device='cuda:0'), tensor(0.3550, device='cuda:0'), tensor(0.3638, device='cuda:0'), tensor(0.3334, device='cuda:0'), tensor(0.3588, device='cuda:0'), tensor(0.3198, device='cuda:0'), tensor(0.3604, device='cuda:0'), tensor(0.3367, device='cuda:0'), tensor(0.3588, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.3404, device='cuda:0'), tensor(0.3234, device='cuda:0'), tensor(0.3544, device='cuda:0'), tensor(0.3386, device='cuda:0'), tensor(0.3446, device='cuda:0'), tensor(0.3559, device='cuda:0'), tensor(0.3363, device='cuda:0'), tensor(0.1973, device='cuda:0'), tensor(0.3530, device='cuda:0'), tensor(0.2141, device='cuda:0'), tensor(0.3691, device='cuda:0'), tensor(0.3005, device='cuda:0'), tensor(0.3582, device='cuda:0'), tensor(0.3284, device='cuda:0'), tensor(0.3548, device='cuda:0'), tensor(0.3500, device='cuda:0'), tensor(0.3631, device='cuda:0'), tensor(0.3511, device='cuda:0'), tensor(0.3405, device='cuda:0'), tensor(0.3539, device='cuda:0'), tensor(0.3523, device='cuda:0'), tensor(0.3631, device='cuda:0'), tensor(0.3565, device='cuda:0'), tensor(0.3314, device='cuda:0'), tensor(0.3544, device='cuda:0'), tensor(0.3752, device='cuda:0'), tensor(0.3429, device='cuda:0'), tensor(0.3572, device='cuda:0'), tensor(0.3460, device='cuda:0'), tensor(0.3684, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.3486, device='cuda:0'), tensor(0.3784, device='cuda:0'), tensor(0.2679, device='cuda:0'), tensor(0.3508, device='cuda:0'), tensor(0.3263, device='cuda:0'), tensor(0.3718, device='cuda:0'), tensor(0.3529, device='cuda:0'), tensor(0.4473, device='cuda:0'), tensor(0.3236, device='cuda:0'), tensor(0.3573, device='cuda:0'), tensor(0.2567, device='cuda:0'), tensor(0.3734, device='cuda:0'), tensor(0.3432, device='cuda:0'), tensor(0.3869, device='cuda:0'), tensor(0.3487, device='cuda:0'), tensor(0.3500, device='cuda:0'), tensor(0.2986, device='cuda:0'), tensor(0.3488, device='cuda:0'), tensor(0.3416, device='cuda:0'), tensor(0.3360, device='cuda:0'), tensor(0.3584, device='cuda:0'), tensor(0.3551, device='cuda:0'), tensor(0.3496, device='cuda:0'), tensor(0.3346, device='cuda:0'), tensor(0.3531, device='cuda:0'), tensor(0.3440, device='cuda:0'), tensor(0.4056, device='cuda:0'), tensor(0.3722, device='cuda:0'), tensor(0.3813, device='cuda:0'), tensor(0.3347, device='cuda:0'), tensor(0.3365, device='cuda:0'), tensor(0.3545, device='cuda:0'), tensor(0.3591, device='cuda:0'), tensor(0.3365, device='cuda:0'), tensor(0.3274, device='cuda:0'), tensor(0.3259, device='cuda:0'), tensor(0.3433, device='cuda:0'), tensor(0.3625, device='cuda:0'), tensor(0.3479, device='cuda:0'), tensor(0.3539, device='cuda:0'), tensor(0.4414, device='cuda:0'), tensor(0.2753, device='cuda:0'), tensor(0.3607, device='cuda:0'), tensor(0.3542, device='cuda:0'), tensor(0.3442, device='cuda:0'), tensor(0.3374, device='cuda:0'), tensor(0.3405, device='cuda:0'), tensor(0.3664, device='cuda:0'), tensor(0.3405, device='cuda:0'), tensor(0.2264, device='cuda:0'), tensor(0.3571, device='cuda:0'), tensor(0.3225, device='cuda:0'), tensor(0.2406, device='cuda:0'), tensor(0.3810, device='cuda:0'), tensor(0.3513, device='cuda:0'), tensor(0.3490, device='cuda:0'), tensor(0.3340, device='cuda:0'), tensor(0.3556, device='cuda:0'), tensor(0.2601, device='cuda:0'), tensor(0.3607, device='cuda:0'), tensor(0.4031, device='cuda:0'), tensor(0.3458, device='cuda:0'), tensor(0.3565, device='cuda:0'), tensor(0.3273, device='cuda:0'), tensor(0.2871, device='cuda:0'), tensor(0.3240, device='cuda:0'), tensor(0.3455, device='cuda:0'), tensor(0.3455, device='cuda:0'), tensor(0.3532, device='cuda:0'), tensor(0.3557, device='cuda:0'), tensor(0.4395, device='cuda:0'), tensor(0.3709, device='cuda:0'), tensor(0.3504, device='cuda:0'), tensor(0.4537, device='cuda:0'), tensor(0.3600, device='cuda:0'), tensor(0.4225, device='cuda:0'), tensor(0.3523, device='cuda:0'), tensor(0.3445, device='cuda:0'), tensor(0.3167, device='cuda:0'), tensor(0.3606, device='cuda:0'), tensor(0.3513, device='cuda:0'), tensor(0.3699, device='cuda:0'), tensor(0.3363, device='cuda:0'), tensor(0.3321, device='cuda:0'), tensor(0.3653, device='cuda:0'), tensor(0.3475, device='cuda:0'), tensor(0.3573, device='cuda:0'), tensor(0.3377, device='cuda:0'), tensor(0.3366, device='cuda:0'), tensor(0.3324, device='cuda:0'), tensor(0.2936, device='cuda:0'), tensor(0.3452, device='cuda:0'), tensor(0.3120, device='cuda:0'), tensor(0.3465, device='cuda:0'), tensor(0.3566, device='cuda:0'), tensor(0.3311, device='cuda:0'), tensor(0.3821, device='cuda:0'), tensor(0.3620, device='cuda:0'), tensor(0.3113, device='cuda:0'), tensor(0.4270, device='cuda:0'), tensor(0.3620, device='cuda:0'), tensor(0.3502, device='cuda:0'), tensor(0.3554, device='cuda:0'), tensor(0.3467, device='cuda:0'), tensor(0.3258, device='cuda:0'), tensor(0.3470, device='cuda:0'), tensor(0.3554, device='cuda:0'), tensor(0.3562, device='cuda:0'), tensor(0.3597, device='cuda:0'), tensor(0.3762, device='cuda:0'), tensor(0.3590, device='cuda:0'), tensor(0.3552, device='cuda:0'), tensor(0.3503, device='cuda:0'), tensor(0.3550, device='cuda:0'), tensor(0.3227, device='cuda:0'), tensor(0.2679, device='cuda:0'), tensor(0.3223, device='cuda:0'), tensor(0.3491, device='cuda:0'), tensor(0.3415, device='cuda:0'), tensor(0.3717, device='cuda:0'), tensor(0.3731, device='cuda:0'), tensor(0.3508, device='cuda:0'), tensor(0.3608, device='cuda:0'), tensor(0.3886, device='cuda:0'), tensor(0.3578, device='cuda:0'), tensor(0.3586, device='cuda:0'), tensor(0.3550, device='cuda:0'), tensor(0.3449, device='cuda:0'), tensor(0.3792, device='cuda:0'), tensor(0.3569, device='cuda:0'), tensor(0.3228, device='cuda:0'), tensor(0.4072, device='cuda:0'), tensor(0.3371, device='cuda:0'), tensor(0.3217, device='cuda:0'), tensor(0.3581, device='cuda:0'), tensor(0.4546, device='cuda:0'), tensor(0.3385, device='cuda:0'), tensor(0.3560, device='cuda:0'), tensor(0.2649, device='cuda:0'), tensor(0.3604, device='cuda:0'), tensor(0.3554, device='cuda:0'), tensor(0.3494, device='cuda:0'), tensor(0.3614, device='cuda:0'), tensor(0.3496, device='cuda:0'), tensor(0.3432, device='cuda:0'), tensor(0.3604, device='cuda:0'), tensor(0.3475, device='cuda:0'), tensor(0.3374, device='cuda:0'), tensor(0.3607, device='cuda:0'), tensor(0.3680, device='cuda:0'), tensor(0.3355, device='cuda:0'), tensor(0.4532, device='cuda:0'), tensor(0.3231, device='cuda:0'), tensor(0.3577, device='cuda:0'), tensor(0.3521, device='cuda:0'), tensor(0.3501, device='cuda:0'), tensor(0.2792, device='cuda:0'), tensor(0.3401, device='cuda:0'), tensor(0.3554, device='cuda:0'), tensor(0.3495, device='cuda:0'), tensor(0.3288, device='cuda:0'), tensor(0.3556, device='cuda:0'), tensor(0.3644, device='cuda:0'), tensor(0.4005, device='cuda:0'), tensor(0.4002, device='cuda:0'), tensor(0.3407, device='cuda:0'), tensor(0.3510, device='cuda:0'), tensor(0.3540, device='cuda:0'), tensor(0.3451, device='cuda:0'), tensor(0.3378, device='cuda:0'), tensor(0.3527, device='cuda:0'), tensor(0.3617, device='cuda:0'), tensor(0.3564, device='cuda:0'), tensor(0.3537, device='cuda:0'), tensor(0.3540, device='cuda:0'), tensor(0.3534, device='cuda:0'), tensor(0.3438, device='cuda:0'), tensor(0.3806, device='cuda:0'), tensor(0.3537, device='cuda:0'), tensor(0.3466, device='cuda:0'), tensor(0.4116, device='cuda:0'), tensor(0.3392, device='cuda:0'), tensor(0.3485, device='cuda:0'), tensor(0.3555, device='cuda:0'), tensor(0.3490, device='cuda:0'), tensor(0.3182, device='cuda:0'), tensor(0.3469, device='cuda:0'), tensor(0.2848, device='cuda:0'), tensor(0.4377, device='cuda:0'), tensor(0.3511, device='cuda:0'), tensor(0.3133, device='cuda:0'), tensor(0.3518, device='cuda:0'), tensor(0.3703, device='cuda:0'), tensor(0.3607, device='cuda:0'), tensor(0.3541, device='cuda:0'), tensor(0.4139, device='cuda:0'), tensor(0.3026, device='cuda:0'), tensor(0.3572, device='cuda:0'), tensor(0.3513, device='cuda:0'), tensor(0.3580, device='cuda:0'), tensor(0.3586, device='cuda:0'), tensor(0.3270, device='cuda:0'), tensor(0.4403, device='cuda:0'), tensor(0.3510, device='cuda:0'), tensor(0.3499, device='cuda:0'), tensor(0.3564, device='cuda:0'), tensor(0.3540, device='cuda:0'), tensor(0.3442, device='cuda:0'), tensor(0.3024, device='cuda:0'), tensor(0.3342, device='cuda:0'), tensor(0.3128, device='cuda:0'), tensor(0.2859, device='cuda:0'), tensor(0.3409, device='cuda:0'), tensor(0.3174, device='cuda:0'), tensor(0.3476, device='cuda:0'), tensor(0.3594, device='cuda:0'), tensor(0.2142, device='cuda:0'), tensor(0.3900, device='cuda:0'), tensor(0.2826, device='cuda:0'), tensor(0.3659, device='cuda:0'), tensor(0.3556, device='cuda:0'), tensor(0.3716, device='cuda:0'), tensor(0.3531, device='cuda:0'), tensor(0.3360, device='cuda:0'), tensor(0.3271, device='cuda:0'), tensor(0.3574, device='cuda:0'), tensor(0.3578, device='cuda:0'), tensor(0.3214, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.3403, device='cuda:0'), tensor(0.2888, device='cuda:0'), tensor(0.3350, device='cuda:0'), tensor(0.3410, device='cuda:0'), tensor(0.3443, device='cuda:0'), tensor(0.3447, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.2631, device='cuda:0'), tensor(0.4560, device='cuda:0'), tensor(0.3561, device='cuda:0'), tensor(0.3579, device='cuda:0'), tensor(0.3775, device='cuda:0'), tensor(0.3430, device='cuda:0'), tensor(0.3553, device='cuda:0'), tensor(0.3372, device='cuda:0'), tensor(0.3567, device='cuda:0'), tensor(0.3408, device='cuda:0'), tensor(0.3573, device='cuda:0'), tensor(0.3540, device='cuda:0'), tensor(0.3927, device='cuda:0'), tensor(0.3455, device='cuda:0'), tensor(0.3461, device='cuda:0'), tensor(0.3554, device='cuda:0'), tensor(0.3376, device='cuda:0'), tensor(0.3610, device='cuda:0'), tensor(0.3476, device='cuda:0'), tensor(0.3605, device='cuda:0'), tensor(0.3557, device='cuda:0'), tensor(0.3424, device='cuda:0'), tensor(0.3578, device='cuda:0'), tensor(0.3567, device='cuda:0'), tensor(0.3501, device='cuda:0'), tensor(0.3454, device='cuda:0'), tensor(0.3346, device='cuda:0'), tensor(0.3507, device='cuda:0'), tensor(0.3079, device='cuda:0'), tensor(0.4262, device='cuda:0'), tensor(0.3568, device='cuda:0'), tensor(0.3662, device='cuda:0'), tensor(0.3492, device='cuda:0'), tensor(0.3472, device='cuda:0'), tensor(0.3635, device='cuda:0'), tensor(0.3424, device='cuda:0'), tensor(0.2593, device='cuda:0'), tensor(0.3580, device='cuda:0'), tensor(0.3277, device='cuda:0'), tensor(0.3628, device='cuda:0'), tensor(0.3148, device='cuda:0'), tensor(0.3220, device='cuda:0'), tensor(0.2839, device='cuda:0'), tensor(0.3745, device='cuda:0'), tensor(0.3435, device='cuda:0'), tensor(0.3601, device='cuda:0'), tensor(0.3671, device='cuda:0'), tensor(0.3537, device='cuda:0'), tensor(0.3729, device='cuda:0'), tensor(0.3215, device='cuda:0'), tensor(0.3855, device='cuda:0'), tensor(0.3416, device='cuda:0'), tensor(0.3332, device='cuda:0'), tensor(0.3519, device='cuda:0'), tensor(0.3118, device='cuda:0'), tensor(0.3639, device='cuda:0'), tensor(0.3586, device='cuda:0'), tensor(0.3228, device='cuda:0'), tensor(0.3424, device='cuda:0'), tensor(0.3423, device='cuda:0'), tensor(0.3734, device='cuda:0'), tensor(0.3516, device='cuda:0'), tensor(0.2491, device='cuda:0'), tensor(0.3360, device='cuda:0'), tensor(0.3659, device='cuda:0'), tensor(0.3431, device='cuda:0'), tensor(0.3753, device='cuda:0'), tensor(0.3507, device='cuda:0'), tensor(0.3736, device='cuda:0'), tensor(0.3393, device='cuda:0'), tensor(0.3525, device='cuda:0'), tensor(0.3616, device='cuda:0'), tensor(0.3442, device='cuda:0'), tensor(0.3439, device='cuda:0'), tensor(0.3297, device='cuda:0'), tensor(0.3537, device='cuda:0'), tensor(0.3426, device='cuda:0'), tensor(0.3891, device='cuda:0'), tensor(0.3588, device='cuda:0'), tensor(0.3291, device='cuda:0'), tensor(0.3416, device='cuda:0'), tensor(0.3294, device='cuda:0'), tensor(0.3485, device='cuda:0'), tensor(0.3988, device='cuda:0'), tensor(0.3603, device='cuda:0'), tensor(0.3497, device='cuda:0'), tensor(0.3579, device='cuda:0'), tensor(0.3356, device='cuda:0'), tensor(0.3053, device='cuda:0'), tensor(0.3906, device='cuda:0'), tensor(0.3446, device='cuda:0'), tensor(0.3614, device='cuda:0'), tensor(0.3501, device='cuda:0'), tensor(0.3470, device='cuda:0'), tensor(0.3564, device='cuda:0'), tensor(0.3577, device='cuda:0'), tensor(0.3705, device='cuda:0'), tensor(0.3351, device='cuda:0'), tensor(0.3499, device='cuda:0'), tensor(0.3573, device='cuda:0'), tensor(0.3457, device='cuda:0'), tensor(0.3310, device='cuda:0'), tensor(0.3442, device='cuda:0'), tensor(0.3852, device='cuda:0'), tensor(0.3183, device='cuda:0'), tensor(0.2689, device='cuda:0'), tensor(0.3930, device='cuda:0'), tensor(0.3073, device='cuda:0'), tensor(0.3231, device='cuda:0'), tensor(0.3533, device='cuda:0'), tensor(0.3794, device='cuda:0'), tensor(0.3370, device='cuda:0'), tensor(0.3569, device='cuda:0'), tensor(0.3480, device='cuda:0'), tensor(0.3500, device='cuda:0'), tensor(0.3526, device='cuda:0'), tensor(0.3451, device='cuda:0'), tensor(0.3601, device='cuda:0'), tensor(0.3591, device='cuda:0'), tensor(0.3542, device='cuda:0'), tensor(0.4092, device='cuda:0'), tensor(0.3456, device='cuda:0'), tensor(0.3393, device='cuda:0'), tensor(0.3337, device='cuda:0'), tensor(0.3546, device='cuda:0'), tensor(0.3127, device='cuda:0'), tensor(0.3539, device='cuda:0'), tensor(0.3408, device='cuda:0'), tensor(0.3518, device='cuda:0'), tensor(0.3332, device='cuda:0'), tensor(0.3466, device='cuda:0'), tensor(0.3153, device='cuda:0'), tensor(0.3629, device='cuda:0'), tensor(0.2492, device='cuda:0'), tensor(0.3442, device='cuda:0'), tensor(0.3740, device='cuda:0'), tensor(0.3516, device='cuda:0'), tensor(0.3497, device='cuda:0'), tensor(0.3237, device='cuda:0'), tensor(0.3579, device='cuda:0'), tensor(0.3514, device='cuda:0'), tensor(0.3419, device='cuda:0'), tensor(0.4117, device='cuda:0'), tensor(0.3555, device='cuda:0'), tensor(0.3145, device='cuda:0'), tensor(0.3680, device='cuda:0'), tensor(0.3235, device='cuda:0'), tensor(0.3930, device='cuda:0'), tensor(0.3665, device='cuda:0'), tensor(0.3424, device='cuda:0'), tensor(0.3202, device='cuda:0'), tensor(0.3483, device='cuda:0'), tensor(0.3430, device='cuda:0'), tensor(0.3793, device='cuda:0'), tensor(0.4350, device='cuda:0'), tensor(0.3487, device='cuda:0'), tensor(0.3372, device='cuda:0'), tensor(0.3474, device='cuda:0'), tensor(0.3596, device='cuda:0'), tensor(0.3502, device='cuda:0'), tensor(0.3799, device='cuda:0'), tensor(0.3534, device='cuda:0'), tensor(0.2984, device='cuda:0'), tensor(0.3603, device='cuda:0'), tensor(0.3554, device='cuda:0'), tensor(0.3158, device='cuda:0'), tensor(0.3468, device='cuda:0'), tensor(0.3576, device='cuda:0'), tensor(0.2732, device='cuda:0'), tensor(0.3364, device='cuda:0'), tensor(0.3059, device='cuda:0'), tensor(0.3851, device='cuda:0'), tensor(0.3240, device='cuda:0'), tensor(0.2000, device='cuda:0'), tensor(0.3570, device='cuda:0'), tensor(0.3437, device='cuda:0'), tensor(0.3487, device='cuda:0'), tensor(0.3325, device='cuda:0'), tensor(0.3557, device='cuda:0'), tensor(0.3195, device='cuda:0'), tensor(0.3474, device='cuda:0'), tensor(0.3343, device='cuda:0'), tensor(0.3620, device='cuda:0'), tensor(0.3305, device='cuda:0'), tensor(0.3576, device='cuda:0'), tensor(0.3411, device='cuda:0'), tensor(0.3449, device='cuda:0'), tensor(0.3582, device='cuda:0'), tensor(0.3529, device='cuda:0'), tensor(0.3501, device='cuda:0'), tensor(0.3475, device='cuda:0'), tensor(0.3531, device='cuda:0'), tensor(0.3433, device='cuda:0'), tensor(0.3486, device='cuda:0'), tensor(0.2912, device='cuda:0'), tensor(0.3491, device='cuda:0'), tensor(0.3348, device='cuda:0'), tensor(0.3208, device='cuda:0'), tensor(0.3176, device='cuda:0'), tensor(0.2983, device='cuda:0'), tensor(0.3487, device='cuda:0'), tensor(0.3516, device='cuda:0'), tensor(0.3450, device='cuda:0'), tensor(0.3621, device='cuda:0'), tensor(0.3561, device='cuda:0'), tensor(0.3709, device='cuda:0'), tensor(0.3589, device='cuda:0'), tensor(0.3834, device='cuda:0'), tensor(0.3542, device='cuda:0'), tensor(0.3642, device='cuda:0'), tensor(0.3559, device='cuda:0'), tensor(0.3453, device='cuda:0'), tensor(0.4592, device='cuda:0'), tensor(0.3575, device='cuda:0'), tensor(0.3875, device='cuda:0'), tensor(0.3608, device='cuda:0'), tensor(0.3512, device='cuda:0'), tensor(0.3551, device='cuda:0'), tensor(0.3770, device='cuda:0'), tensor(0.4066, device='cuda:0'), tensor(0.3540, device='cuda:0'), tensor(0.3407, device='cuda:0'), tensor(0.3551, device='cuda:0'), tensor(0.2243, device='cuda:0'), tensor(0.3165, device='cuda:0'), tensor(0.3665, device='cuda:0'), tensor(0.3622, device='cuda:0'), tensor(0.3532, device='cuda:0'), tensor(0.3765, device='cuda:0'), tensor(0.3576, device='cuda:0'), tensor(0.3712, device='cuda:0'), tensor(0.3415, device='cuda:0'), tensor(0.4035, device='cuda:0'), tensor(0.3423, device='cuda:0'), tensor(0.3229, device='cuda:0'), tensor(0.3574, device='cuda:0'), tensor(0.3208, device='cuda:0'), tensor(0.3659, device='cuda:0'), tensor(0.3615, device='cuda:0'), tensor(0.1949, device='cuda:0'), tensor(0.4049, device='cuda:0'), tensor(0.3477, device='cuda:0'), tensor(0.4288, device='cuda:0'), tensor(0.3691, device='cuda:0'), tensor(0.3485, device='cuda:0'), tensor(0.3279, device='cuda:0'), tensor(0.3629, device='cuda:0'), tensor(0.2774, device='cuda:0'), tensor(0.3657, device='cuda:0'), tensor(0.3293, device='cuda:0'), tensor(0.3592, device='cuda:0'), tensor(0.3392, device='cuda:0'), tensor(0.3399, device='cuda:0'), tensor(0.3102, device='cuda:0'), tensor(0.3900, device='cuda:0'), tensor(0.3645, device='cuda:0'), tensor(0.3696, device='cuda:0'), tensor(0.3399, device='cuda:0'), tensor(0.3446, device='cuda:0'), tensor(0.3397, device='cuda:0'), tensor(0.3482, device='cuda:0'), tensor(0.3673, device='cuda:0'), tensor(0.3608, device='cuda:0'), tensor(0.3423, device='cuda:0'), tensor(0.3693, device='cuda:0'), tensor(0.2919, device='cuda:0'), tensor(0.3216, device='cuda:0'), tensor(0.3512, device='cuda:0'), tensor(0.3899, device='cuda:0'), tensor(0.3512, device='cuda:0'), tensor(0.3618, device='cuda:0'), tensor(0.3620, device='cuda:0'), tensor(0.3557, device='cuda:0'), tensor(0.3514, device='cuda:0'), tensor(0.3497, device='cuda:0'), tensor(0.3061, device='cuda:0'), tensor(0.3533, device='cuda:0'), tensor(0.3146, device='cuda:0'), tensor(0.3482, device='cuda:0'), tensor(0.3580, device='cuda:0'), tensor(0.4309, device='cuda:0'), tensor(0.3517, device='cuda:0'), tensor(0.3572, device='cuda:0'), tensor(0.3241, device='cuda:0'), tensor(0.3421, device='cuda:0'), tensor(0.3546, device='cuda:0'), tensor(0.3015, device='cuda:0'), tensor(0.3644, device='cuda:0'), tensor(0.3491, device='cuda:0'), tensor(0.3575, device='cuda:0'), tensor(0.3451, device='cuda:0'), tensor(0.3463, device='cuda:0'), tensor(0.2465, device='cuda:0'), tensor(0.3344, device='cuda:0'), tensor(0.3446, device='cuda:0'), tensor(0.3584, device='cuda:0'), tensor(0.3380, device='cuda:0'), tensor(0.3549, device='cuda:0'), tensor(0.3507, device='cuda:0'), tensor(0.3557, device='cuda:0'), tensor(0.3430, device='cuda:0'), tensor(0.3467, device='cuda:0'), tensor(0.3527, device='cuda:0'), tensor(0.4238, device='cuda:0'), tensor(0.3481, device='cuda:0'), tensor(0.3470, device='cuda:0'), tensor(0.3616, device='cuda:0'), tensor(0.3438, device='cuda:0'), tensor(0.3548, device='cuda:0'), tensor(0.3512, device='cuda:0'), tensor(0.3259, device='cuda:0'), tensor(0.3570, device='cuda:0'), tensor(0.3490, device='cuda:0'), tensor(0.3239, device='cuda:0'), tensor(0.3167, device='cuda:0'), tensor(0.3588, device='cuda:0'), tensor(0.3499, device='cuda:0'), tensor(0.3450, device='cuda:0'), tensor(0.3368, device='cuda:0'), tensor(0.3725, device='cuda:0'), tensor(0.3403, device='cuda:0'), tensor(0.3510, device='cuda:0'), tensor(0.3524, device='cuda:0'), tensor(0.3511, device='cuda:0'), tensor(0.3556, device='cuda:0'), tensor(0.3995, device='cuda:0'), tensor(0.3511, device='cuda:0'), tensor(0.3219, device='cuda:0'), tensor(0.3514, device='cuda:0'), tensor(0.3418, device='cuda:0'), tensor(0.3522, device='cuda:0'), tensor(0.3410, device='cuda:0'), tensor(0.3083, device='cuda:0'), tensor(0.3411, device='cuda:0'), tensor(0.3673, device='cuda:0'), tensor(0.3622, device='cuda:0'), tensor(0.3784, device='cuda:0'), tensor(0.3630, device='cuda:0'), tensor(0.2991, device='cuda:0'), tensor(0.3498, device='cuda:0'), tensor(0.3344, device='cuda:0'), tensor(0.2423, device='cuda:0'), tensor(0.3425, device='cuda:0'), tensor(0.3342, device='cuda:0'), tensor(0.3658, device='cuda:0'), tensor(0.4143, device='cuda:0'), tensor(0.2452, device='cuda:0'), tensor(0.3545, device='cuda:0'), tensor(0.2672, device='cuda:0'), tensor(0.3527, device='cuda:0'), tensor(0.3533, device='cuda:0'), tensor(0.3321, device='cuda:0'), tensor(0.4775, device='cuda:0'), tensor(0.3483, device='cuda:0'), tensor(0.3298, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(results_list)\n",
    "# Plot schedular graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "a = []\n",
    "for i in range(len(loss_track)):\n",
    "    a.append(i)\n",
    "\n",
    "plt.plot(range(len(a)), a, label=\"Training Loss\")\n",
    "\n",
    "# plt.plot(range(len(loss_track_valid)), loss_track_valid, label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_{}\".format(\"first_test\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
